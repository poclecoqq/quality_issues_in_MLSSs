Interviewer 1: Thank you. All right, thank you. All right. So to start off, uh, can you, uh, tell us a bit about yourself, how many years of experience you have in machine learning and in 

Interviewee: general? Yeah. Uh, well, I'm an engineer system. I got my degree on 2012. Um, but actually I started working in, um, business intelligence, uh, area.

Interviewee: Um, and like, I don't know, No, actually four years ago I started like moving, uh, towards, um, machine learning. I made two post degrees. One was in big data and another one that I just finished, uh, the past year, uh, that was, uh, artificial intelligence. And right now, right now I'm doing like, um, a master, uh, also in big data, but it's actually related to machine learning.

Interviewee: So that's my. My background so far,

Interviewee: not You're muted. Yeah. Sorry. 

Interviewer 1: I was, uh, I was saying, uh, we are happy to have you with us. 

Interviewee: Hmm. Thank you. 

Interviewer 1: Uh, so to start off, can you give us, uh, well, tell us what are the main quality issues you have encountered? What your what data model or system so 

Interviewee: far? Yeah. I think the main problem that I, I usually. Is the lack of information about machine learning, uh, from the, the customer.

Interviewee: You know, it's like, um, they don't, they, uh, they, they know nothing or mostly nothing. And then you have to introduce everything, all the concepts, uh, things that you need to do in order to get like, uh, for them, like a good model or a quality model or whatever. And. And sometimes they think that, I don't know, you're, uh, kind of wizard, you know, like just doing ma uh, magic with the things that they give to you.

Interviewee: And, um, and I think that's the problem, uh, that, uh, persists the data because actually they're the ones giving you the data, but, um, they don't know why, why are you asking for data or, um, why, uh, such data with such characteristics or whatever. I think that the problem starts with the client not knowing what is machine learning or, um, with, I mean, no idea of the task involved in that process.

Interviewee: I think everything starts there. I 

Interviewer 1: see. Thank you. Um, have you ever used data that was manually collected? 

Interviewee: No, not manually, no. Usually I, I mean, they give me data from databases or I don't know, sources that, that, um, they pull data from databases, um, or I don't know, public sources for images or whatever. But yeah, no manually collected data, no.

Interviewer 1: Oh, okay. And so you, you talk about public sources, uh mm-hmm. , what are the main quality issues you have with this kind of data? 

Interviewee: Um, usually the bad labeling, uh, for data. Okay. You know, um, I remember for instance, doing, um, Celebrity, recognizer, uh, system. And yeah, we have like a bunch of faces that actually, uh, were from the couple of the person, or I don't know, was, uh, his sister or her sister, or her brother.

Interviewee: Or his brother. Uh, and uh, yeah, in those cases, uh, with public sources, the promise is always like, uh, have good labeling or labeling, sorry, uh, of the. I 

Interviewer 1: see. And how do you address this problem? Uh, do you have some tool to help you detect mislabel? 

Interviewee: Yeah. Yeah. We need to run like a pro, a clean process, you know, like trying to get all the faces and see which were like the old layers, and then manually check each of them and start like cleaning everything.

Interviewee: But yeah, I mean it's like a, uh, a long process to, to run, of course. Mm-hmm. . 

Interviewer 1: And have you ever used data that was generated by another? Sorry, come again. Have you ever used a data that was generated by another system? 

Interviewee: Mm, I mean, I mean the, the data that comes from database is always generated by a system. I don't know.

Interviewee: What do you mean by a assist by another system? Uh, 

Interviewer 1: so for example, like previously we mentioned, um, we mentioned like minor only collected data. Uh, but you, you could have a, for example, a transaction system that records every transaction. Uh, well, if you're in a company that sells product to customers, uh, and by recording the transaction well, uh, during generating data like a log, you know, uh, so, so this, this is my question.

Interviewer 1: If you ever use this type of data that basically it's not a human who entered, who created the, the sample. 

Interviewee: Yeah. Yeah. Let me. Uh, yeah, I mean the, the, I have used, uh, some data that actually is part of transaction or, or a process of some transactions. And, uh, and yeah, yeah, I have used that. Uh, I mean, I, I haven't seen any problem on that or, or actually I have.

Interviewee: But, um, for instance, uh, we once have a client that. Uh, the client, uh, gave us some data, uh, but actually when we started like analyzing the data, we start to see like a bunch of weird things and, uh, The problem was that, um, that they have like a nightly job that run and I don't know, um, was trying to conciliate, uh, the different sources that they, they have in a, in a, in a data, in a database.

Interviewee: And for some mistake, uh, the, were a couple of fields that, uh, they were like empty and. And yeah, I mean, um, they realized that when we start announcing data and, uh mm-hmm. , we told them, Hey guys, I mean, uh, these fields are empty. Why They're empty. Uh, we see like this pattern on this data. And yeah, they, they realized about that after I see, after analysis.

Interviewee: Yeah. Thank 

Interviewer 1: you. Um, how do you, well, have you ever measured the quality of your data and or tried to improve?

Interviewee: Measure it? No, I think I haven't measure it. Uh, because again, um, you, you always try to like, depending on the problem that you're trying to resolve, you know, uh, which, which data you need and how much, of course, and you try to get to something with that. But measuring quality sometimes is hard because you need to know like, Or, or, or be like pretty close to the business to understand which are the problems, uh, that could occur on that data.

Interviewee: And that sometimes is just impossible because again, in my case, for instance, I'm usually like some sort of outsourcing, uh, for companies. So I never get too close to the, the sources from the data. Uh, and also, I'm not too. Close to the business persons. So it's hard to, to notice like, uh, how good is the data or how, how bad it is.

Interviewee: We just, again, run some analysis to see out layers missing data. I mean, usually things that you do to, before running like, like a machine learning model, but to like, I think, to really understand how good it is. I need, I think you need like to have like a very. I mean, to be pretty close to the business to understand like, um, in a greater degree how good it is.

Interviewee: Um, that's, that's, that's what I think. 

Interviewer 1: Yeah. It's, uh, it's a, it's pretty good point. Yeah. Um, is there any other data quality should we miss that you consider relevant? 

Interviewee: Mm. No, usually, I mean, the ones that I have faced was, was, yeah, bad. Uh, labeling, um, bad quality data. Like, again, missing fields. Um, all I mean, like fields that should not be like with certain values, but actually they have it.

Interviewee: Um, Uh, again, problem with the, with the client understanding what is machine learning and all the process related. Um,

Interviewee: yeah, nothing that I can think of right now. Yeah.

Interviewer 1: Sorry, I was taking note of, uh, something. No, it's okay. Regarding the interview. Yeah. Um, Yes. Sorry. Uh, how do you evaluate the quality of model? And as a reminder, quality is not only defined by the ML performance, F1 score accuracy. Mm-hmm. But there's also other aspects such as scalability, uh, efficiency, uh, robustness, explainability, and there's other, yeah.

Interviewee: Mm-hmm. and, sorry, what, what was the question? Uh, how 

Interviewer 1: do you evaluate the quality of your models? 

Interviewee: Oh, got it. No, in, in my case, I usually, it, it, again, it depends on the client, because the client's going to tell you, uh, what they need. I mean, I, I always ask, ask this question if they want to have some interpretability for the model, if they need to know why the model predicted this or whatever.

Interviewee: So, After you do like this questionnaire, I mean, you know, uh, for which, uh, path go, right? I mean, because if you want to have interpretability, probably you are going to discard. Uh, neural networks, uh, if they want to have something that, uh, must be performance, I don't know per, you're going to see some light models or whatever and see if, if they can run or know in some, uh, small devices or whatever.

Interviewee: Also depends when they're going to be. Uh, sorry. Um, yeah, where they going to be running this models if they need to run this in a cell phone, in a computer, um, and or whatever. So all the things that you ask, uh, eventually determine the things that you're going to try or use for, for defining the, the model and.

Interviewee: I guess that is going to like give you like the quality of a model based on that framework that you build on top of those requirements. Um, Usually what I mean when we have all this, uh, all this information from the client. Yeah. We, we try to run, uh, all the process for the data cleaning, whatever, uh, new features.

Interviewee: Uh, and then we start like building the, the machine learning model. And then we also have the phase when you, we have to deploy it and you also have to take in care, uh, sorry to take, uh, that into. Because, yeah. Um, sometimes they need like things like, uh, real time or near real time, so you need to take into account, uh, the inference time as well.

Interviewee: And also, if you are going to employ that in a, I don't know, in a web server or whatever, the api, you also have to take into account the latency from the request. And when the request comes back and. Again, I mean, there are so many things between you start and when you end, and also the thing doesn't end there because you always have to think what's going to happen later.

Interviewee: I mean, because, um, machine learning models tend to be, uh, or, or I mean with the time, the, the performance tends to, uh, de de.

Interviewee: Sorry, I can recall. Degrees. Yeah. Degrees, yeah. Degrees and uh, And that implies that you need to somehow track the performance of the model, um, in time in. And if you see something is going, um, with some troubles or whatever, you need to like trigger like another retrain with newer data, um, and so on. So I think it's not, uh, it's, I mean, the quality of the model is not just a.

Interviewee: Or, or, or a part of a process. I think it must be something that, uh, it's related to the whole process and, uh, in this like infinite cycle of train and retraining and deploying and whatever. So I think ev, I mean all that process, uh, makes part of the quality of the, of the model or, or the solution, I would say, because the model is just the model, you know, I would like to say the solution as part of something greater.

Interviewer 1: That, that's a really good answer, really comprehensive answer. Thank you. Uh, so you mentioned a few interesting points I'd like to, to go deeper on mm-hmm. into, uh, so you mentioned, you mentioned, uh, sometime you have to consider the size of your model or of the application because it's gonna go on a, on a Edge device.

Interviewer 1: Mm-hmm. . Uh, so, uh, would you give me some example and also tell me how you manage, how you, well, what you do to decrease the size of your application or. . 

Interviewee: Yeah. The, for, for instance, uh, I remember once we have a client that, I mean, we never did nothing for this client, but actually we have some discussions and actually he had some, I mean his, so, I mean his product was based on edge devices, I think was a camera like pointing into, um, uh, I don't know if a refrigerator or so.

Interviewee: And trying to count the things that, uh, were taken from, I mean, from the customers, you know, like, uh, I know this client took a Coke, this client took a, I don't know, a piece of, um, I don't know, whatever. So, um, he, I mean, he needed to run like this algorithm to try to, um, keep the stock up to date and. Yeah, in those cases, you, again, if you're going to run some in edge devices, uh, there are just a few solution that are suitable for, for that.

Interviewee: Uh, for instance, uh, you have to use like light versions for, for that because you need like, um, good inference. Also, you need to see how good is the model, uh, with certain quality of the image. Not all, I mean not, uh, all algorithms can take. Uh, good embeddings depending on the image. So you, you have to try like, uh, a lot of them.

Interviewee: Um, then, uh, there are also some, some things that you can do. Uh, I mean that we will, uh, try to research that if you have like a big model you have, you can actually. Um, do some sort of compression when you, when you, I mean, when you run like this model and you convert it to something lighter. Um, actually we never did, but I mean, we research about it and it was possible.

Interviewee: Um, what else? And, uh,

Interviewee: Mm. Yeah. Also, I mean also in there are like these, uh, these other, I mean, sometimes probably, uh, there's, there is no. Uh, on, I mean, there's, there's no one single model. Probably you need more like, uh, one model to, uh, run, like, uh, classification on the image. Based on that, you pass it to, I don't know, to, uh, sh uh, uh, yellow algorithm to detect some objects and whatever.

Interviewee: And, and you also have to consider the, the whole process. because it's not the just one model. You have like a chain of models, and that's also important to take into account. . So, uh, when you have all these, again, you need to start like, uh, seeing when you, uh, sorry, where you can, uh, improve or, or, uh, make, uh, model lighter or make a model faster for inference or whatever.

Interviewee: I mean, you start to tweak, um, depending on what you need. Uh, usually that's the thing that you, you have to do. . 

Interviewer 1: Mm-hmm. . And, uh, you also mentioned Latin, Latin issues. Mm-hmm. . Uh, so I'm gonna ask you the same, same question. Can you give me an example and also how you solve that problem? 

Interviewee: No, in, in that case, I mean, was to just, um, make the model run faster for inference, you know, because, uh, Usually the, the latency program is not such a big thing nowadays.

Interviewee: I mean, uh, with the capabilities of internet that we have and the, all the 5G nets or 4G nets or whatever. I mean, that's not an issue. Um, so you have to mostly work on the, on the model side for inference or, or, or, or, or the, or the process piece that you need to do in order to. Um, bring the data to the model and the model, make the inference because I mean, sometimes not the model, the, the problem probably is the pre-processing phase that you do for the data.

Interviewee: Uh, and again, you, you need to see where is, uh, where you're wasting time on that and see if you can, uh, improve that. Uh, so yeah, usually latency is related to that. I mean, some part of, of your request is taking too much and to get the response back, and then you need to see what is done is in the problem of sending the image or the data that you need or whatever is the problem of processing it is the problem with the model genetic interference.

Interviewee: Probably in the post, um, in fairness, probably you need to do more things or change that, uh, uh, response with other models and so on. So, yeah, sometimes it can be very complex, sometimes cannot, but yeah. Okay. It, it, 

Interviewer 1: it means, seems obvious, but why is it complex to, to find. Latency issues in, in the pipeline from, uh, the data pipeline to the model, to the post processing.

Interviewer 1: What is it, why is it complex? What is a challenge? No, 

Interviewee: sorry, I I'm not saying that. Find the promise complex. I, I, I, I was saying that the, the, the chain can be complex. I mean, Uh, but no, usually what you do is like tracking, like, uh, again, I mean usually we do like, um, uh, how it's called this, uh, stress test, you know, and then you see how much load can the, the system can, can hold and yeah, just start to see all the requests.

Interviewee: How much time did, um, uh, do they take? And then you start like, like, Like doing the, some sort of debugging. If you, if you, if you like to call that way, uh, and see piece by piece which of the, which piece of the code is taking the most, and based on that, then you, you start like doing your guesses and Yeah.

Interviewee: Try to, uh, improve that based on that. Right. Yeah. Yeah. 

Interviewer 1: Thank you. Um, have you ever assessed the quality of a model prediction with the user of the system? Mm. 

Interviewee: Like what? Um, 

Interviewer 1: um, I don't know. Uh, let's say, let's say, let's take your, um, example earlier, earlier on with the camera that films the, 

Interviewee: the, the fridge.

Interviewee: Mm-hmm. . 

Interviewer 1: Well, let, let's say that the user are the people that retrieve things from the fridge. Uh, maybe they will give you feedback like it doesn't. Uh, ah, got, got it. This, yeah. This is an example. 

Interviewee: Yeah. No, in our case, I mean, um, for, for, for instance, for this, um, for this model that I, that I was saying that we were like recognizing, uh, celebrities, uh, I mean the, the, there wasn't like a feedback mechanism.

Interviewee: Uh, actually it was the, the, the customers from the client, um, uh, complain about the results. , but yeah, no, no, no. Like, uh, like, uh, like a good system of feedback mechanism that you get like the, the things from the customers and in a, like, in a fashion way, you know, it was just complaints about the, the, the clients from, from, oh, sorry.

Interviewee: The customers from our. Okay. And they, they 

Interviewer 1: complained about like, uh, ML performance, like mm-hmm. 

Interviewee: bad prediction. Okay. Yeah. Yeah. I mean, it wasn't, I mean, when we really measured the things, I think it was like 1% of the time, like, um, Like, I don't know, tires or bottles being recognized as, as faces. Um, and again, it, it wasn't as bad as, as the, uh, as the, uh, the client was, uh, uh, sane.

Interviewee: But I mean, and, and we show, we showed them, I mean, yeah, I know. I mean, the D is not working well here because again, I. I see the picture and I see a tire. Of course, it's, it's, it's obvious for me, but again, this is, uh, uh, an ML model. I mean, ML models makes mistakes. They are not, I mean, they are never going to be like 100% sure.

Interviewee: And of course, they, they handle like, uh, great volume of, uh, pictures and faces. And if you start thinking like, I don't know, 90% of the time, uh, you are right, but that 1% in uh, 100 is 10 in 1000 is 100. And that keep going up, you know? So, uh, mistakes are going to happen. I mean, you have to deal with it. And again, we come to discussion, uh, that, uh, when clients doesn't understand these things or these concepts, , you start to seeing like these discussions, uh, that probably could be easier if they un understand better the, the concept we have behind, uh, machine learning.

Interviewee: Mm-hmm. . 

Interviewer 1: Mm-hmm. . Yes. Um, what are the challenges you have encountered during the deployment of a machine learning software system? Um, 

Interviewee: Usually I don't have much of issues, uh, but in, in this client that, that I, that I'm actually working right now. Uh, the problem was, uh, that for instance for me, or, I mean for my team, the best thing always is to make like a docker image and we give that to the client and they deploy it whatever they want.

Interviewee: If it is Azure, uh, a w s or whatever, I mean, I think it's easiest way to deploy something. Um, and actually we discussed this with the. I mean, we, we, we, uh, we told them, I mean, um, we have the solution. This API that's going to be built in, um, in Python using Fast api. It's going to be pretty easy. I mean, we are going to give you like a Docker image you have just to deploy it to, uh, whatever you want.

Interviewee: And that's it. And they agreed. And, uh, at some point, or we finish with the solution, um, we have, uh, the docker image, everything ready. And so they like, uh, schedule a meeting, uh, between us and they, and they are in infrastructure team. And when we came to the meeting, uh, Was like some, I mean, it was funny because actually they said, yeah, we don't work with Python, we don't use Docker, we don't use nothing that you have you guys built.

Interviewee: So it was like, what? And uh, and actually we have to pull some resources from our company to help them to use stock or in nash because I mean, they have all, uh, and they, they. Infrastructure in Azure, but they were, were using like in, in the old fashioned way, you know, like building web services and all, uh, just from the, from scratch.

Interviewee: And they were not using docker images. So yeah, we have to pull some sources from our side to help them to use that docker images and, um, yeah, make all the deployment or, and so on. But yeah, I think that was the only issue that I, I. Uh, faced with, with deployment, uh, and yeah, again, was was funny because, um, the, I mean, the business persons agreed, but yeah, they, they weren't aware that, uh, the infrastructure team or the dev op team, uh, didn't know anything about, uh, uh, Docker.

Interviewee: Yeah. 

Interviewer 1: Yeah. . Uh, alright. Um, what are the challenges if, if you, yeah, what are the challenges that you have encountered during the maintenance of a machine learning software system? 

Interviewee: Uh, Mm. Usually the the problem is not the mi the maintenance, it's the, to, to get like the, the approval for implementing the feedback.

Interviewee: MEChA, sorry, feedback mechanism. I mean, usually the clients don't want to waste time on, on. On, on the model. I mean, to, to, to, I mean, to mon monitor the model and also waste time on check if it is performing well or, or whatever. They just want to move on to the next problem that, that they have. I mean, it's just a, yeah, we already solve this problem with this M ML model.

Interviewee: Renee is working well. Okay, let's move on to the next thing. And the problem is usually to convince them that. Hey guys. I mean, this is going to, I mean, pro is not, um, the day tomorrow Pro is not the next month. Pro is not the next year. But at some point, this model is going to need to retrain, be retrained it.

Interviewee: Uh, and for that we need to track the performance of the model. We need to somehow, uh, get feedback, feedback from the user that you're using, the model, uh, and so on. But yeah, again, uh, they don't want to waste. Or unmanly on that. Uh, they just want to put the, uh, focus on on other issues that they are having at the moment.

Interviewee: And, uh, yeah, I think usually that's the problem, to convince the, the clients to, to implement a feedback mechanism, uh, and, uh, yeah. Make that happen is sometimes can be very hard. Yeah. Hmm. 

Interviewer 1: That's, that's new information. Thank you. Um, Sorry, I'm, I'm just, uh, screening if, uh, yeah, sure. Because we already covered some questions, so, uh, um, yes.

Interviewer 1: Have you en, have you encountered issues with data, uh, during the maintenance machine learning software system? 

Interviewee: No, not so far, no. Okay, perfect. No, not that I can remember. No. Okay. And or the model. No, no. I mean, I mean, for instance, um, the client that we are working right now, uh, they were using like a model that has like, I don't know, 20 years more or less.

Interviewee: And, uh, when they told us that the model, uh, was there like for 20 years, not retrained. Not retrained, I mean, um, like a pretty old. And actually they didn't have any information about the model. They didn't know how the model was trained. How was, uh, I mean, which I mean, they know which fields, uh, were used.

Interviewee: I mean, at least for prediction, you don't know if that was also, uh, used for training or whatever. But yeah, I mean, uh, the like information was like a real problem. And, uh, also they didn't. Any way to track the performance. I mean, they were like blindly, uh, blindly trusting of, of the prediction from the model for 20 years, and they actually don't know if it, it's performing well or no.

Interviewee: Or not, sorry. And, uh, yeah, one thing that we actually did was to retrain a new model based on that. And, um, of course we did all the recommendation of all the process, all the steps that we take, that we took, sorry. Um, all the algorithm that we try, all the metrics that we collected for the, for the algorithms, all the, I don't.

Interviewee: I mean, uh, we try to document everything because again, you never know who's going to come later after you. So yeah, we, we didn't, um, want to make the same mistakes that, uh, this person from, I, I don't know which company, uh, made, uh, when, when he built this, this model. So, Yeah, I think, I mean, it is not something, it is not something that I have faced, but I, I have, I, I have faced with this, uh, with this, uh, in this situation.

Interviewee: I mean, like seeing all the problems that, uh, they, uh, have because of, uh, again, lack of documentation, um, lack of, uh, feedback mechanism, lack. Perform, I mean like, um, lack of, um, this, uh, performance monitoring and so on. So, uh, um, luckily it wasn't me, but I mean, the one that, uh, did that . But yeah, uh, that's, uh, a thing that, uh, I noticed with, with this project or with this kind, actually.

Interviewee: Thank you. 

Interviewer 1: Um, and interesting, uh, to share that knowledge. Um, so I, I will list you a couple of quality aspect and if you ever had issue with one of them, uh, feel free to mention your experience. Okay? So fair fairness, robustness, explainability, scalability, privacy, and security. Well, data privacy and model.

Interviewee: No, I am, I, I would agree with all of them. Um, um, of course there are some qualities that are, um, I mean that, that imply more work than others. For instance, the furnace. I mean, uh, that's sometimes something, uh, hard to measure and to check if your model somehow has some. Between, I know, graces or, uh, or some ethnic of peoples.

Interviewee: Um, and also of course, uh, it's, it's what the probably what, where the client, uh, don't want to spend time or money, uh, trying to improve that. I mean, um,

Interviewee: Yeah, because the rest of the one you mentioned, and yeah, security, of course, it's important for the client. Uh, you mentioned it, which, which one? Uh, yeah. Um, data privacy. Data privacy, of course. Uh, scalability. Explainability. Scalability. Yeah. Yeah. Yeah. Explainability mostly depends on the, on the industry.

Interviewee: Uh, I guess that, again, that depends on the customer. for instance, if you probably, uh, work with healthcare, uh, y they, they want to see why the model is predicting, uh, cancer for this image, or it's not predicting cancer for this image. You know, it's like they are very. I mean, they, they do not trust too much on the, on the predictions.

Interviewee: And you somehow have to show them, uh, why the model is, uh, saying what, uh, it's saying. And yeah, when you, I don't know, use like this algorithm, like sharp, that you, you know, like, uh, it gives you like some region of the image that the machine learning is, it's looking at when making the prediction and they see, oh, okay.

Interviewee: Yeah. It's, it's the same version that I would. I will look at for, uh, breaking this or saying if this is, uh, a normal cancer or whatever, uh, they start like, uh, trust a little bit more in the model and they are, uh, more willing to, uh, using it and so on.

Interviewer 1: Oh, sorry. All right. Yes. Thank you. Um, yeah, I think, um, No, no, it's, it's all right. I, I had, I had a follow up question, but, um, I forgot it. So anyway, yeah. Uh, so I have two more questions for you. Um, in your opinion. In your opinion, mm-hmm. , what is the most pressing quality issue researchers should, researchers should try to solve, 

Interviewee: uh, quality issues that researchers should mostly solve?

Interviewee: Um,

Interviewee: Anything that you don't have any effective solution for that? Anything, or you need, you think that should be investigated more? That, yeah. Hmm, hmm. I think, I think, um, or I would like to, for instance, uh, what is so-called the, um, ma machinal learning. Uh, with the, I mean with all this problem with, with privacy data, you know, and the idea that you can, uh, ask for these companies to delete your data from the models or, or, or at least the information that they, they used for train the models.

Interviewee: And, um, of course, I mean, the models need to unlearn from your. But they should keep like, uh, having like the same performance even, uh, uh, even after losing that information or piece of information that you took out of them. Um, I think that's an interesting, I, I'm sorry. An interesting thing. Um, Uh, because again, um, I think nowadays people is like, uh, getting some, uh, kind of consciousness about, uh, all the data that we are giving to Google, Facebook, uh, Instagram, whatever.

Interviewee: And they're starting realizing how much, uh, how much these companies, uh, knows about us. And, uh, I think that's an an, I mean, an inter interest. Point of research because I think, um, with time people are going to be asking to be, I mean, all his data or her data or data be to be the leader from these models.

Interviewee: And I think that's something that we should took into account to, to research how machines can unlearn what I, what they have learned from our data. Is 

Interviewer 1: there, is it something that you have experienced in your, uh, 

Interviewee: Build model. Okay. No, no. But, but it is interesting. I read, I, I, I read an article and I was very interested by, by, by it.

Interviewee: Yeah. Yeah. 

Interviewer 1: Uh, do you have any other comment about the quality of machine learning 

Interviewee: software system? Mm,

Interviewee: no. I think, no. I mean, um, I dunno. If I think a little bit more, I, I could, uh, come up with something else, but I think we already have discussed most of them, or at least the most important ones. Yeah. 

Interviewer 1: All right. Well thank you. I think you provided a lot of good points. It will be really, uh, useful for our study.

Interviewer 1: Uh, 

Interviewee: thanks a lot. No, thank you. Good luck with that. Thank you. Alright. Thank you for your, Thank you. Thank you. Bye. See you. Bye bye.

