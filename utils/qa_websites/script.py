from typing import Dict, List
import requests
import json
from time import sleep

from utils import init_logger, get_key, save_file, save_to_csv, load_file

logger = init_logger()


def get_top_users_id_per_tag(website: str, tags: List, n_results=100, verbose=False) -> Dict:
    """
    Gets top users' id for a set of tags for a given website.
    Top users a fetched from two categories: "all_time" and "month".
    Top users are also divided in two categories: "answerers" and "askers".
    Input:
        website_url: The stack exchange website on which we will search for users.
        tags: The tags for which we want the top users.
        n_results: Number of users fetched per category (period x user_type)
    Output:
        users: a dict of list of user ids, where the keys are generated by the "get_key" function.
    """
    ans = {}
    for tag in tags:
        if verbose:
            logger.info(
                f'Fetching top users on "{website}" for the tag "{tag}."')
        params = {'pagesize': n_results, 'site': website}
        for user_category in ['top-askers', 'top-answerers']:
            for period in ['all_time', 'month']:
                url = f'https://api.stackexchange.com/2.3/tags/{tag}/{user_category}/{period}'
                res = requests.get(url, params=params)
                # To avoid hitting Stack Exchange's throttling limits
                sleep(1)
                parsed_response = json.loads(res.text)
                # logger.info if verbose
                if verbose:
                    n_users = len(parsed_response['items'])
                    logger.info(
                        f'\t{user_category}-{period} number of users: {n_users}')
                # Extract the users id from the results
                user_ids = []
                for user in parsed_response['items']:
                    user_ids.append(user['user']['user_id'])
                k = get_key(tag, user_category, period)
                ans[k] = user_ids
    return ans


def split_list(l: List, size) -> List:
    """
    Split a list in a list of list of max length "size"
    Input:
        l: the list too be splitted
        size: the max size
    Output:
        ans: ...
    """
    i = 0
    ans = []
    while i < len(l):
        ans.append(l[i:i+size])
        i += size
    return ans


def get_users_web_handles(website: str, ids: List, n_results=100, verbose=False) -> List:
    """
    Given a list of user ids, fetches user handles
    """
    base_url = 'https://api.stackexchange.com/2.3/users/'
    params = {'pagesize': n_results, 'site': website}
    splitted_ids = split_list(ids, size=100)
    user_handles = []
    for s_ids in splitted_ids:
        s_ids = list(map(str, s_ids))
        joined_ids = ';'.join(s_ids)
        url = base_url + joined_ids
        res = requests.get(url, params=params)
        sleep(1)
        parsed_response = json.loads(res.text)
        # Extract the users id from the results
        for user in parsed_response['items']:
            if 'website_url' in user and len(user['website_url']) > 0:
                user_handles.append(user['website_url'])
    if verbose:
        n_user_ids = len(ids)
        n_user_handles = len(user_handles)
        logger.info(
            f'Fetched {n_user_handles} user handles from {n_user_ids} user ids from {website}.')
    return user_handles


def get_top_users_web_handles(website: str, n_results=100, verbose=False) -> List:
    """
    Fetches the web handle of top users of a platform.
    Input:
        website: The website
        n_results: 
        verbose: 
    Output:
        user_handles: A list of user handles        
    """
    url = 'https://api.stackexchange.com/2.3/users'
    params = {'pagesize': n_results, 'site': website}
    res = requests.get(url, params=params)
    sleep(1)
    parsed_response = json.loads(res.text)

    # Extract the users id from the results
    user_handles = []
    for user in parsed_response['items']:
        if 'website_url' in user and len(user['website_url']) > 0:
            user_handles.append(user['website_url'])

    if verbose:
        n_user_ids = len(parsed_response['items'])
        n_user_handles = len(user_handles)
        logger.info(
            f'Fetched {n_user_handles} user handles from {n_user_ids} user ids from {website}.')

    return user_handles


def get_web_handles_datascience_stack_exchange(ans: Dict) -> Dict:
    """
    Fetches web handles from datascience stack exchange.
    Input:
        ans: The dict where the answer will be saved
    Output:
        ans: The same dict as given in input, but with data saved in it
    """
    verbose = True
    if verbose:
        logger.info("\n----------- Data Science Stack Exchange -----------")
    website = 'datascience'
    website_handles = get_top_users_web_handles(
        website=website, verbose=verbose)
    k = get_key(website)
    ans[k] = website_handles
    return ans


def get_web_handles_datascience_stack_overflow(ans: Dict, use_cached_user_ids=False) -> Dict:
    """
    Fetches web handles from stackoverflow.
    First it searches top users per tags.
    Second it fetches the web handle of these users.
    Input:
        ans: The dict where the answer will be saved
    Output:
        ans: The same dict as given in input, but with data saved in it
    """
    verbose = True
    if verbose:
        logger.info("\n----------- Stack Overflow -----------")
    website = 'stackoverflow'
    f_cached_name = 'SO-user-ids.bin'
    # 1. Fetch user ids if not using cached ones (if existing)
    if not use_cached_user_ids:
        tags = ['machine-learning', 'data-cleaning',
                'dataset', 'artificial-intelligence']
        # Get the top users for each tag
        users_id = get_top_users_id_per_tag(
            website=website, tags=tags, verbose=verbose)
        save_file(f_cached_name, users_id)
    else:
        users_id = load_file(f_cached_name)

    # 2. Fetch the web handles of these users
    for key, _users_id in users_id.items():
        if verbose:
            logger.terminator = ":\t"
            logger.info(key)
            logger.terminator = "\n"
        website_handles = get_users_web_handles(
            website=website, ids=_users_id, verbose=verbose)
        ans[key] = website_handles
    return ans


if __name__ == "__main__":
    ans = {}
    ans = get_web_handles_datascience_stack_overflow(ans)
    save_file('tmp_final.bin', ans)
    ans = get_web_handles_datascience_stack_exchange(ans)
    save_to_csv('final.csv', ans)
